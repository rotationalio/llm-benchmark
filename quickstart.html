

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Getting Started &mdash; Construe v0.4.0-beta.4 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
    <link rel="canonical" href="https://construe.rotational.dev/quickstart.html" />
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=24bb955b"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Benchmarks" href="benchmarks.html" />
    <link rel="prev" title="Construe: An LLM Benchmark Utility" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Construe
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Getting Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#installation">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#basic-usage">Basic Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="#run-benchmarks">Run Benchmarks</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data-storage">Data Storage</a></li>
<li class="toctree-l2"><a class="reference internal" href="#basic-benchmarks">Basic Benchmarks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="benchmarks.html">Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="developers.html">Development Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/index.html">API Reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Construe</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Getting Started</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/quickstart.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="getting-started">
<h1>Getting Started<a class="headerlink" href="#getting-started" title="Link to this heading"></a></h1>
<p>Construe is designed to be quickly installed on an Ubuntu environment with Internet connectivity using <code class="docutils literal notranslate"><span class="pre">pip</span></code> and the benchmarks run quickly downloading models and datasets from a cloud bucket and cleaning them up to minimize disk space utilization. At the conclusion of the benchmarks, an output JSON file with the results is saved to disk.</p>
<p>To quickly run the benchmarks:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ pip install construe
$ construe -e &quot;device-name&quot; run
</pre></div>
</div>
<p>Where “device-name” is the name of the device or experiment that you’re running the benchmarks on.</p>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Link to this heading"></a></h2>
<p>This package is intended to be installed with <code class="docutils literal notranslate"><span class="pre">pip</span></code> and it will create a command line program <code class="docutils literal notranslate"><span class="pre">construe</span></code> on your <code class="docutils literal notranslate"><span class="pre">$PATH</span></code> to execute benchmarking comamnds:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ pip install -U construe
$ which construe
$ construe --version
</pre></div>
</div>
<p>It is perfectly fine to install <code class="docutils literal notranslate"><span class="pre">construe</span></code> into a virtual environment (in fact it is recommended). If you get a <code class="docutils literal notranslate"><span class="pre">pip</span></code> error about not being able to resolve dependencies; please <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">uninstall</span></code> all of the specified dependencies and try installing <code class="docutils literal notranslate"><span class="pre">construe</span></code> again.</p>
</section>
<section id="basic-usage">
<h2>Basic Usage<a class="headerlink" href="#basic-usage" title="Link to this heading"></a></h2>
<p>There are several top-level configurations that you can specify either as an environment variable or a command line option before the command. The environment variables are as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">$CONSTRUE_ENV</span></code> or <code class="docutils literal notranslate"><span class="pre">$ENV</span></code>: specify the name of the experimental environment for comparison purposes usually the name of the device (by default the hostname is used).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">$CONSTRUE_DEVICE</span></code> or <code class="docutils literal notranslate"><span class="pre">$TORCH_DEVICE</span></code>: specify the name of the default accelerator to use with PyTorch e.g. cpu, mps, or cuda (this does not influence tflite).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">$CONSTRUE_DATA</span></code>: the path to download datasets to for temporary storage during the benchmarks.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">$CONSTRUE_MODELS</span></code>: the path to download models to for temporary storage during the benchmarks.</p></li>
</ul>
<p>The command line utility help is as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ construe --help
Usage: construe [OPTIONS] COMMAND [ARGS]...

  A utility for executing inferencing benchmarks.

Options:
  --version                     Show the version and exit.
  -o, --out TEXT                specify the path to write the benchmark
                                results to
  -d, --device TEXT             specify the pytorch device to run on e.g. cpu,
                                mps or cuda
  -e, --env TEXT                name of the experimental environment for
                                comparison (default is hostname)
  -c, --count INTEGER           specify the number of times to run each
                                benchmark
  -l, --limit INTEGER           limit the number of instances to inference on
                                in each benchmark
  -D, --datadir TEXT            specify the location to download datasets to
  -M, --modeldir TEXT           specify the location to download models to
  -S, --sample / --no-sample    use sample dataset instead of full dataset for
                                benchmark
  -C, --cleanup / --no-cleanup  cleanup all downloaded datasets after the
                                benchmark is run
  -Q, --verbose / --quiet       specify the verbosity of the output and
                                progress bars
  -h, --help                    Show this message and exit.

Commands:
  basic      Runs basic dot product performance benchmarks.
  datasets   Helper utility for managing the dataset cache.
  gliner     Executes GLiNER named entity discovery inferencing benchmarks.
  lowlight   Executes lowlight image enhancement inferencing benchmarks.
  mobilenet  Executes image classification inferencing benchmarks.
  mobilevit  Executes object detection inferencing benchmarks.
  models     Helper utility for managing the models cache.
  moondream  Executes image-to-text inferencing benchmarks.
  nsfw       Executes NSFW image classification inferencing benchmarks.
  offensive  Executes offensive speech text classification inferencing...
  run        Executes all available benchmarks.
  whisper    Executes audio-to-text inferencing benchmarks.
</pre></div>
</div>
</section>
<section id="run-benchmarks">
<h2>Run Benchmarks<a class="headerlink" href="#run-benchmarks" title="Link to this heading"></a></h2>
<p>You can either run all available benchamrks (excluding some, or specifying which benchmarks to include) or you can run an individual benchmark. To run all benchmarks:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ construe -e &quot;MacBook Pro 2022 M1&quot; run
</pre></div>
</div>
<p>This will run all available benchmarks. The <code class="docutils literal notranslate"><span class="pre">-e</span></code> flag specifies the environment for comparison purposes and the results will be saved as a JSON file on the local disk.</p>
<p>When running each benchmark, the model and dataset for that benchmark is downloaded, the benchmark is executed, then the model and dataset are cleaned up. If you do not want the data to be cleaned up use the <code class="docutils literal notranslate"><span class="pre">-C</span></code> or <code class="docutils literal notranslate"><span class="pre">--no-cleanup</span></code> flag to cache the models and datasets between runs.</p>
<p>If you would like to limit the number of instances per run you can use the <code class="docutils literal notranslate"><span class="pre">-l</span></code> or <code class="docutils literal notranslate"><span class="pre">--limit</span></code> flag; this might speed up the benchmarks if you’re just trying to get a simple sense of inferening on the device. You can also specify the <code class="docutils literal notranslate"><span class="pre">-c</span></code> or <code class="docutils literal notranslate"><span class="pre">--count</span></code> flag to run each benchmark multiple times on the same instances to get more detailed results.</p>
<p>To run an individual benchmark, run it by name; for example to run the <code class="docutils literal notranslate"><span class="pre">whisper</span></code> speech-to-text benchmark:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ construe whisper
</pre></div>
</div>
<p>Alternatively if you want to exclude <code class="docutils literal notranslate"><span class="pre">whisper</span></code> (e.g. run all benchmarks but <code class="docutils literal notranslate"><span class="pre">whisper</span></code>), use the <code class="docutils literal notranslate"><span class="pre">-E</span></code> or <code class="docutils literal notranslate"><span class="pre">--exclude</span></code> flag as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ construe run -E whisper
</pre></div>
</div>
</section>
<section id="data-storage">
<h2>Data Storage<a class="headerlink" href="#data-storage" title="Link to this heading"></a></h2>
<p>The benchmarks download both models and datasets (samples by default) from the cloud before executing the benchmark, then deletes the models and datasets after the benchmark is run to preserve device space. By default, data is stored in <code class="docutils literal notranslate"><span class="pre">$HOME/.construe</span></code>, however, construe allows you to configure where the models and data are stored so that you can specify a volume that has enough storage space.</p>
<p>To specify the location where the downloaded data and models are stored, use either the <code class="docutils literal notranslate"><span class="pre">-D</span></code> and <code class="docutils literal notranslate"><span class="pre">-M</span></code> flags with <code class="docutils literal notranslate"><span class="pre">construe</span></code> as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ construe -D /path/to/data -M /path/to/models run
</pre></div>
</div>
<p>Or set the <code class="docutils literal notranslate"><span class="pre">$CONSTRUE_DATA</span></code> and <code class="docutils literal notranslate"><span class="pre">$CONSTRUE_MODELS</span></code> environment variables with the specified paths.</p>
<p>You can determine where the model and datasets are stored byh using the following commands:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ construe datasets
/home/user/.construe/data
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ construe models
/home/user/.construe/models
</pre></div>
</div>
<p>These commands also allow you to download the models and datasets before running the benchmarks. This is often preferable to ensure multiple runs of the benchmarks cache the data. Use the <code class="docutils literal notranslate"><span class="pre">--no-cleanup</span></code> flag with <code class="docutils literal notranslate"><span class="pre">construe</span></code> to ensure that manually downloaded models and datasets are not cleaned up after the benchmarks are run.</p>
</section>
<section id="basic-benchmarks">
<h2>Basic Benchmarks<a class="headerlink" href="#basic-benchmarks" title="Link to this heading"></a></h2>
<p>The basic benchmarks implement dot product benchmarks from the <a class="reference external" href="https://pytorch.org/tutorials/recipes/recipes/benchmark.html">PyTorch documentation</a>. These benchmarks can be run using <code class="docutils literal notranslate"><span class="pre">construe</span> <span class="pre">basic</span></code>; for example by running:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ construe -e &quot;MacBook Pro 2022 M1&quot; basic -o results-macbook.pickle
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">-e</span></code> flag specifies the environment for comparison purposes and the <code class="docutils literal notranslate"><span class="pre">-o</span></code> flag saves the measurements out to disk as a Pickle file that can be loaded for comparison to other environments later.</p>
<p>Command usage is as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Usage</span><span class="p">:</span> <span class="n">construe</span> <span class="n">basic</span> <span class="p">[</span><span class="n">OPTIONS</span><span class="p">]</span>

<span class="n">Options</span><span class="p">:</span>
  <span class="o">-</span><span class="n">e</span><span class="p">,</span> <span class="o">--</span><span class="n">env</span> <span class="n">TEXT</span>             <span class="n">name</span> <span class="n">of</span> <span class="n">the</span> <span class="n">experimental</span> <span class="n">environment</span> <span class="k">for</span>
                             <span class="n">comparison</span> <span class="p">(</span><span class="n">default</span> <span class="ow">is</span> <span class="n">hostname</span><span class="p">)</span>
  <span class="o">-</span><span class="n">o</span><span class="p">,</span> <span class="o">--</span><span class="n">saveto</span> <span class="n">TEXT</span>          <span class="n">path</span> <span class="n">to</span> <span class="n">write</span> <span class="n">the</span> <span class="n">measurements</span> <span class="n">pickle</span> <span class="n">data</span> <span class="n">to</span>
  <span class="o">-</span><span class="n">t</span><span class="p">,</span> <span class="o">--</span><span class="n">num</span><span class="o">-</span><span class="n">threads</span> <span class="n">INTEGER</span>  <span class="n">specify</span> <span class="n">number</span> <span class="n">of</span> <span class="n">threads</span> <span class="k">for</span> <span class="n">benchmark</span> <span class="p">(</span><span class="n">default</span>
                             <span class="n">to</span> <span class="n">maximum</span><span class="p">)</span>
  <span class="o">-</span><span class="n">F</span><span class="p">,</span> <span class="o">--</span><span class="n">fuzz</span> <span class="o">/</span> <span class="o">--</span><span class="n">no</span><span class="o">-</span><span class="n">fuzz</span>     <span class="n">fuzz</span> <span class="n">the</span> <span class="n">tensor</span> <span class="n">sizes</span> <span class="n">of</span> <span class="n">the</span> <span class="n">inputs</span> <span class="n">to</span> <span class="n">the</span>
                             <span class="n">benchmark</span>
  <span class="o">-</span><span class="n">S</span><span class="p">,</span> <span class="o">--</span><span class="n">seed</span> <span class="n">INTEGER</span>         <span class="nb">set</span> <span class="n">the</span> <span class="n">random</span> <span class="n">seed</span> <span class="k">for</span> <span class="n">random</span> <span class="n">generation</span>
  <span class="o">-</span><span class="n">h</span><span class="p">,</span> <span class="o">--</span><span class="n">help</span>                 <span class="n">Show</span> <span class="n">this</span> <span class="n">message</span> <span class="ow">and</span> <span class="n">exit</span><span class="o">.</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Construe: An LLM Benchmark Utility" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="benchmarks.html" class="btn btn-neutral float-right" title="Benchmarks" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Rotational Labs.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>